### created and maintained by nick@redhat.com
### This script assumes you are running the install from the master node.  VANILLA RHEL 7
### source:
### https://docs.openshift.com/container-platform/3.7/install_config/install/prerequisites.html
#
# HTTPS whitelist these hosts if you are behind a proxy.
## for RHSM/RHN (rpms)
subscription.rhn.redhat.com
## for RH’s docker registry
registry.access.redhat.com
access.redhat.com

# BEGIN
## do this on ALL hosts (master/infra/nodes).  copy and paste between the <BREAK>s
### SET THESE VARIABLES ###
export POOLID=
export ROOT_DOMAIN=ocp.nicknach.net
export APPS_DOMAIN=apps.$ROOT_DOMAIN 
export OCP_USER=nicknach
export OCP_PASSWD=welcome1
export RHSM_ID=
export RHSM_PW=
export DOCKER_DEV=/dev/vdb
export OCP_NFS_MOUNT=/home/data/openshift
export OCP_NFS_SERVER=storage.home.nicknach.net
export LDAP_SERVER=gw.home.nicknach.net

# make them persistent 
cat <<EOF >> ~/.bashrc
export ROOT_DOMAIN=$ROOT_DOMAIN
export APPS_DOMAIN=$APPS_DOMAIN
export OCP_USER=$OCP_USER
export OCP_PASSWD=$OCP_PASSWD
export RHSM_ID=$OCP_RHSM_ID
export RHSM_PW=$RHSM_PW
export DOCKER_DEV=$DOCKER_DEV
export OCP_NFS_MOUNT=$OCP_NFS_MOUNT
export OCP_NFS_SERVER=$OCP_NFS_SERVER
export LDAP_SERVER=$LDAP_SERVER
EOF

# subscribe to RHSM
#yum install subscription-manager yum-utils -y
#subscription-manager register --username=$RHSM_ID --password $RHSM_PW --force
#subscription-manager attach --pool=$POOLID
#subscription-manager repos --disable="*"
#subscription-manager repos \
#   --enable=rhel-7-server-rpms \
#   --enable=rhel-7-server-extras-rpms \
#   --enable=rhel-7-server-ose-3.7-rpms \
#   --enable=rhel-7-fast-datapath-rpms \
#   --enable=rhel-7-server-rhscl-rpms \
#   --enable=rhel-7-server-optional-rpms \
#   --enable=rh-gluster-3-for-rhel-7-server-rpms \
#   --enable=rhel-7-server-3scale-amp-2.0-rpms

# Or, if you have internal repos
yum-config-manager --disable "*" && rm -rf /etc/yum.repos.d/*.repo
#yum-config-manager --add-repo http://repo.home.nicknach.net/repo/rhel-7-server-rpms
yum-config-manager --add-repo http://repo.home.nicknach.net/repo/rhel-7-server-extras-rpms
yum-config-manager --add-repo http://repo.home.nicknach.net/repo/rhaos-3.9
echo gpgcheck=0 >> /etc/yum.repos.d/repo.home.nicknach.net_repo_rhaos-3.9.repo
#yum-config-manager --add-repo http://repo.home.nicknach.net/repo/rhel-7-fast-datapath-rpms
#yum-config-manager --add-repo http://repo.home.nicknach.net/repo/rhel-server-rhscl-7-rpms
#yum-config-manager --add-repo http://repo.home.nicknach.net/repo/rhel-7-server-optional-rpms
#yum-config-manager --add-repo http://repo.home.nicknach.net/repo/rh-gluster-3-for-rhel-7-server-rpms
#yum-config-manager --add-repo http://repo.home.nicknach.net/repo/epel
#yum-config-manager --add-repo http://repo.home.nicknach.net/repo/cuda
#yum-config-manager --add-repo http://repo.home.nicknach.net/repo/rhel-7-server-3scale-amp-2.0-rpms

## install some general pre-req packages 
yum install -y yum-utils wget git net-tools bind-utils iptables-services bridge-utils bash-completion nfs-utils dstat mlocate

## install openshift specific pre-reqs
yum install -y atomic atomic-openshift-utils openshift-ansible atomic-openshift-clients

## install docker (non-Atomic installs)
yum install -y docker docker-logrotate

## install gluster packages
#yum -y install cns-deploy heketi-client

### if using 3scale
#yum install 3scale-amp-template -y

## switch to a local registry mirror


#yum update -y 
# reboot

## configure the docker pool device
cat <<EOF > /etc/sysconfig/docker-storage-setup
DEVS=$DOCKER_DEV
VG=docker-vg
WIPE_SIGNATURES=true
EOF
# and setup the storage
docker-storage-setup

# <BREAK>
# On main master only now
## make passwordless key for ose installer usage
ssh-keygen
# <BREAK> copy keys to all hosts(masters/nodes)
## make a list.txt of public IPs and then do...
for i in `cat list.txt`; do ssh-copy-id root@$i; done
# create your ansible hosts (inventory) file 
# (see other doc for creating this file)
# <BREAK>
## now run the ansible playbook to install
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml
# if you to need explicitly provide a private keyfile (like with AWS)
--private-key ~/.ssh/nick-west2.pem
## verify the install was successful (oc get nodes)

# <BREAK> 
## aliases for ops
echo alias allpods=\'watch -n1 oc adm manage-node --selector="" --list-pods -owide\' > /etc/profile.d/ocp.sh
echo alias allpodsp=\'watch -n1 oc adm manage-node --selector="region=primary" --list-pods -owide\' >> /etc/profile.d/ocp.sh
chmod +x /etc/profile.d/ocp.sh

## pin all the metrics pods to the infra nodes
oc patch ns openshift-infra -p '{"metadata": {"annotations": {"openshift.io/node-selector": "region=infra"}}}'

## set gluster to be the default storageclass
oc annotate storageclass glusterfs-storage storageclass.beta.kubernetes.io/is-default-class="true"

## setup group sync and run it once
wget https://raw.githubusercontent.com/nnachefski/ocpstuff/master/ocp_group_sync.conf -O /etc/origin/master/ocp_group_sync.conf
wget https://raw.githubusercontent.com/nnachefski/ocpstuff/master/ocp_group_sync-whitelist.conf -O /etc/origin/master/ocp_group_sync-whitelist.conf 
oc adm groups sync --sync-config=/etc/origin/master/ocp_group_sync.conf --confirm --whitelist=/etc/origin/master/ocp_group_sync-whitelist.conf

### setup a cronjob on ansible host to sync groups nightly
echo 'oc adm groups sync --sync-config=/etc/origin/master/ocp_group_sync.conf --confirm --whitelist=/etc/origin/master/ocp_group_sync-whitelist.conf' > /etc/cron.daily/ocp-group-sync.sh && chmod +x /etc/cron.daily/ocp-group-sync.sh 

### set policies (perms) on your sync’ed groups
oc adm policy add-cluster-role-to-group cluster-admin admins
oc adm policy add-role-to-group basic-user authenticated

## entitle admins group to run pods as root
#oc adm policy add-scc-to-group anyuid system:admins

## entitle the current project’s default svcaccount to run as anyuid
#oc adm policy add-scc-to-user anyuid -z default

### setup a cronjob on ansible host to prune images
echo 'ansible all -m shell -a "docker rm \$(docker ps -q -f status=exited); docker volume rm \$(docker volume ls -qf dangling=true); docker rmi \$(docker images --filter "dangling=true" -q --no-trunc)"' >> /etc/cron.daily/ocp-image-clean.sh && chmod +x /etc/cron.daily/ocp-image-clean.sh

## setup a local htpasswd user if not using LDAP or SSO
#htpasswd -b /etc/origin/master/htpasswd $OCP_USER $OCP_PASSWD
## set user to be cluster-admin role
#oc adm policy add-cluster-role-to-user cluster-admin $OCP_USER

## temporarily set scheduling for masters
#oc adm manage-node --selector="region=infra" --schedulable=true
## create the registry manually
# oc adm registry --create --credentials=/etc/origin/master/openshift-registry.kubeconfig --selector region=infra
#oc adm router --service-account=router --selector region=infra
#oc adm policy add-scc-to-user privileged -z router
#oc adm policy add-scc-to-user hostnetwork -z router
#oc adm policy add-cluster-role-to-user system:router system:serviceaccount:default:router
oc adm manage-node --selector="region=infra" --schedulable=false

### deploying 3scale
#yum install -y 3scale-amp-template
#oc new-project 3scaleamp
#oc new-app --file /opt/amp/templates/amp.yml --param #WILDCARD_DOMAIN=apps.ocp.nicknach.net --param ADMIN_PASSWORD=welcome1

# <BREAK>
## add a storage class if using dynamic provisioning
## for AWS
oc create -f - <<EOF
apiVersion: v1
kind: StorageClass
apiVersion: storage.k8s.io/v1beta1
metadata:
  name: aws-ebs-default
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  zone: us-west-2b
  iopsPerGB: "1000" 
  encrypted: "false"
EOF

## for GCE
oc create -f - <<EOF
apiVersion: v1
kind: StorageClass
apiVersion: storage.k8s.io/v1beta1
metadata:
 name: gce-default
provisioner: kubernetes.io/gce-pd 
parameters:
 type: pd-standard 
 zone: us-west2-b
EOF

# <BREAK>
## create PV for registry (NFS)
oc create -f - <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: registry-volume
spec:
  capacity:
    storage: 40Gi
  accessModes:
  - ReadWriteMany
  nfs:
    path: "$OCP_NFS_MOUNT/enterprise/docker-registry"
    server: "$OCP_NFS_SERVER"
  persistentVolumeReclaimPolicy: Retain
EOF

## create PV for etcd (NFS)
oc create -f - <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: etcd-volume
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    path: "$OCP_NFS_MOUNT/enterprise/etcd"
    server: "$OCP_NFS_SERVER"
  persistentVolumeReclaimPolicy: Retain
EOF


## OR
## create PV for registry (AWS)
oc create -f - <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: registry-volume
spec:
  capacity:
    storage: 50Gi
  accessModes:
  - ReadWriteOnce
  awsElasticBlockStore:
    fsType: ext4
    volumeID: "vol-013223dd81d5b2cfa"
  persistentVolumeReclaimPolicy: Retain
EOF

## OR
## create PV for registry (GCE)
oc create -f - <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: registry-volume
#  failure-domain.beta.kubernetes.io/region: "us-west1" 
#  failure-domain.beta.kubernetes.io/zone: "us-west1-b"
spec:
  capacity:
    storage: 70Gi
  accessModes:
  - ReadWriteOnce
  gcePersistentDisk:
    fsType: ext4
    pdName: "docker-registry"
  persistentVolumeReclaimPolicy: Delete
EOF

## now create the PVC
oc create -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: registry-volume-claim
  labels:
    deploymentconfig: docker-registry
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 40Gi
EOF

## switch the registry’s storage to our NFS-backed PV we just created
oc volume dc docker-registry  --add --overwrite -t persistentVolumeClaim  --claim-name=registry-volume-claim --name=registry-storage

#### Deploy metrics
## switch to metrics project
oc project openshift-infra

## create PV for metrics (NFS)
oc create -f - <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: metrics-volume
spec:
  capacity:
    storage: 30Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    path: "$OCP_NFS_MOUNT/enterprise/metrics"
    server: "$OCP_NFS_SERVER"
  persistentVolumeReclaimPolicy: Retain
EOF

ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-metrics.yml -e openshift_metrics_install_metrics=true -e openshift_metrics_cassandra_storage_type=pv -e openshift_metrics_cassandra_pvc_size=30Gi

-e openshift_metrics_cassandra_storage_type=dynamic 

# <BREAK>
## now once the deployer is finished and you see your pods as “Running” (watch -n1 ‘oc get pods’ also, ‘oc get events’)


# Setup aggregated logging
oc project logging

## create PV for logging (NFS)
oc create -f - <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: logging-volume
spec:
  capacity:
    storage: 20Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    path: "$OCP_NFS_MOUNT/enterprise/logging"
    server: "$OCP_NFS_SERVER"
  persistentVolumeReclaimPolicy: Retain
EOF

ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml -e openshift_logging_install_logging=true -e openshift_logging_es_pvc_size=20Gi


# done
